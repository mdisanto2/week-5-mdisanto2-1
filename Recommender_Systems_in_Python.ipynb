{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems (and their implementations in Python)\n",
    "\n",
    "Recommender systems are among the most popular applications of data science today. They are used to predict the \"rating\" or \"preference\" that a user would give to an item. Almost every major tech company has applied them in some form or the other: Amazon uses it to suggest products to customers, YouTube uses it to decide which video to play next on autoplay, and Facebook uses it to recommend pages to like and people to follow. What's more, for some companies -think Netflix and Spotify-, the business model and its success revolves around the potency of their recommendations. In fact, Netflix even offered a million dollars in 2009 to anyone who could improve its system by 10%.\n",
    "\n",
    "In this tutorial, you will see how to build a basic model of simple as well as content-based recommender systems. While these models will be nowhere close to the industry standard in terms of complexity, quality or accuracy, it will help you to get started with building more complex models that produce even better results.\n",
    "\n",
    "But what are these recommender systems?\n",
    "\n",
    "Broadly, recommender systems can be classified into 3 types:\n",
    "\n",
    "- Simple recommenders: offer generalized recommendations to every user, based on movie popularity and/or genre. The basic idea behind this system is that movies that are more popular and critically acclaimed will have a higher probability of being liked by the average audience. IMDB Top 250 is an example of this system.\n",
    "- Content-based recommenders: suggest similar items based on a particular item. This system uses item metadata, such as genre, director, description, actors, etc. for movies, to make these recommendations. The general idea behind these recommender systems is that if a person liked a particular item, he or she will also like an item that is similar to it.\n",
    "- Collaborative filtering engines: these systems try to predict the rating or preference that a user would give an item-based on past ratings and preferences of other users. Collaborative filters do not require item metadata like its content-based counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Recommenders\n",
    "\n",
    "As described in the previous section, simple recommenders are basic systems that recommends the top items based on a certain metric or score. In this section, you will build a simplified clone of IMDB Top 250 Movies using metadata collected from IMDB.\n",
    "\n",
    "The following are the steps involved:\n",
    "\n",
    "- Decide on the metric or score to rate movies on.\n",
    "- Calculate the score for every movie.\n",
    "- Sort the movies based on the score and output the top results.\n",
    "Before you perform any of the above steps, load your movies metadata dataset into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load Movies Metadata\n",
    "metadata = pd.read_csv('./data/movies_metadata.csv', low_memory=False)\n",
    "\n",
    "# Print the first three rows\n",
    "metadata.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most basic metrics you can think of is the **rating**. However, using this metric has a few *caveats*. For one, it does not take into consideration the **popularity** (which is very important) of a movie. Therefore, a movie with a rating of `9` from `10` voters will be considered `'better'` than a movie with a rating of `8.9` from `10,000` voters.\n",
    "\n",
    "On a related note, this metric will also tend to favor movies with smaller number of voters with skewed and/or extremely high ratings. As the number of voters increase, the rating of a movie *regularizes* and approaches towards a value that is reflective of the movie's quality. It is more difficult to discern the quality of a movie with **extremely** few voters.\n",
    "\n",
    "Taking these shortcomings into consideration, it is necessary that you come up with a **weighted rating** that takes into account the average rating and the number of votes it has garnered. Such a system will make sure that a movie with a `9` rating from `100,000` voters gets a (far) **higher** score than a YouTube Web Series with the same rating but a few hundred voters.\n",
    "\n",
    "Since you are trying to build a clone of `IMDB's Top 250`, you will use its weighted rating formula as your metric/score. Mathematically, $WR$ is represented as follows:\n",
    "\n",
    "$$Weighted Rating (WR) = (\\frac{v}{(v+m)}*R) + (\\frac{m}{(v+m)}*C)$$\n",
    "\n",
    "where,\n",
    "\n",
    "- $v$ is the number of votes for the movie;\n",
    "- $m$ is the minimum votes required to be listed in the chart;\n",
    "- $R$ is the average rating of the movie; And\n",
    "- $C$ is the mean vote across the whole data set.\n",
    "\n",
    "You already have the values to $v$ (`vote_count`) and $R$ (`vote_average`) for each movie in the dataset. It is also possible to directly calculate $C$ from this data.\n",
    "\n",
    "What you need to determine is an appropriate value for $m$, the minimum votes required to be listed in the chart. There is no right value for $m$. You can view it as a ***preliminary negative filter that ignores movies which have less than a certain number of votes***. The selectivity of your filter is up to your discretion (usually done in trial and error).\n",
    "\n",
    "In this case, you will use the `90th` percentile as your cutoff. In other words, for a movie to feature in the charts, it must have more votes than at least `90%` of the movies in the list. (On the other hand, if you had chosen the `75th` percentile, you would have considered the top `25%` of the movies in terms of the number of votes garnered. As the percentile decreases, the number of movies considered increases. Feel free to play with this value and observe the changes in your final chart).\n",
    "\n",
    "As a first step, let's calculate the value of $C$, the mean rating across all movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate C\n",
    "C = metadata['vote_average'].mean()\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average rating of a movie on IMDB is around `5.6`, on a scale of `10`.\n",
    "\n",
    "Next, let's calculate the number of votes, $m$, received by a movie in the `90th` percentile. `Pandas` makes this task extremely trivial using the `.quantile()` method of a pandas Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the minimum number of votes required to be in the chart, m\n",
    "m = metadata['vote_count'].quantile(0.90)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can filter the movies that qualify for the chart, based on their vote counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter out all qualified movies into a new DataFrame\n",
    "q_movies = metadata.copy().loc[metadata['vote_count'] >= m]\n",
    "q_movies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YOUR TURN HERE\n",
    "\n",
    "Answer this question: how many movie have reviews more than 90% of the movies in the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click and type your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use the `.copy()` method to ensure that the new `q_movies` DataFrame created is independent of your original metadata DataFrame. In other words, any changes made to the `q_movies` DataFrame does not affect the metadata.\n",
    "\n",
    "You see that there are 4555 movies which qualify to be in this list. Now, you need to calculate your metric for each qualified movie. To do this, you will define a function, `weighted_rating()` and define a new feature `score`, of which you'll calculate the value by applying this function to your DataFrame of qualified movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that computes the weighted rating of each movie\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['vote_count']\n",
    "    R = x['vote_average']\n",
    "    # Calculation based on the IMDB formula\n",
    "    return (v/(v+m) * R) + (m/(m+v) * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a new feature 'score' and calculate its value with `weighted_rating()`\n",
    "q_movies['score'] = q_movies.apply(weighted_rating, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's sort the DataFrame based on the `score` feature and output the title, vote count, vote average and weighted rating or score of the **top** 15 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sort movies based on score calculated above\n",
    "q_movies = q_movies.sort_values('score', ascending=False)\n",
    "\n",
    "#Print the top 15 movies\n",
    "q_movies[['title', 'vote_count', 'vote_average', 'score']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that the chart has a lot of movies in common with the IMDB Top 250 chart: for example, your top two movies, \"Shawshank Redemption\" and \"The Godfather\", are the same as IMDB. In other words, if anyone search for any movie, these two should be the top 2 being recommended to them.\n",
    "\n",
    "#### YOUR TURN HERE\n",
    "Please try to tweak the $m$ parameter (originally set at `90th percentile`) to a `75th percentile`, and repeat all the steps to calculate the rating of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Define a new `m1` parameter\n",
    "\n",
    "### Filter out all qualified movies into a new DataFrame `my_movies`\n",
    "\n",
    "### apply `weighted_rating()` to `my_movies` to calculate the new `score`\n",
    "\n",
    "###Sort movies based on score calculated above\n",
    "\n",
    "###Print the top 15 movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION:** Are the results different? If so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**: type your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Recommender in Python\n",
    "### Plot Description Based Recommender\n",
    "In this section, you will try to build a system that recommends movies that are similar to a **particular** movie. More specifically, you will compute `pairwise similarity scores` for all movies based on their plot descriptions and recommend movies based on that similarity score. You can see the improvement from the simple recommender system we built above\n",
    "\n",
    "The plot description is available to you as the overview feature in your metadata dataset. Let's inspect the plots of a few movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Print plot overviews of the first 5 movies.\n",
    "metadata['overview'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its current form, it is not possible to compute the similarity between any two overviews. To do this, you need to compute the word vectors of each overview or document, as it will be called from now on.\n",
    "\n",
    "You will compute **Term Frequency-Inverse Document Frequency (TF-IDF)** vectors for each document. This will give you a matrix where each column represents a word in the overview vocabulary (all the words that appear in at least one document) and each column represents a movie, as before. Please note that this is an important step in **text analytics**.\n",
    "\n",
    "In its essence, the TF-IDF score is the frequency of a word occurring in a document, down-weighted by the number of documents in which it occurs. This is done to reduce the importance of words that occur frequently in plot overviews and therefore (e.g. movie, actor - since they have little value in comparing movies), their significance in computing the final similarity score.\n",
    "\n",
    "Fortunately, scikit-learn gives you a built-in `TfIdfVectorizer` class that produces the TF-IDF matrix in a couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import TfIdfVectorizer from scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "#Replace NaN with an empty string\n",
    "metadata['overview'] = metadata['overview'].fillna('')\n",
    "my_metadata = metadata['overview'][:10000]\n",
    "\n",
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(my_metadata)\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that about `75,000` different words were used to describe over `45,000` movies in your dataset.\n",
    "\n",
    "With this matrix in hand, you can now compute a similarity score. There are several candidates for this; such as the `euclidean`, the `Pearson` and the `cosine` similarity scores. Again, there is no right answer to which score is the best. Different scores work well in different scenarios and it is often a good idea to experiment with different metrics.\n",
    "\n",
    "You will be using the `cosine similarity` to calculate a numeric quantity that denotes the similarity between two movies. You use the cosine similarity score since it is independent of magnitude and is relatively easy and fast to calculate (especially when used in conjunction with TF-IDF scores, which will be explained later). Mathematically, it is defined as follows: \n",
    "\n",
    "$$cosine(x,y) = \\frac{x \\cdot y^T}{||x||\\cdot ||y||}$$\n",
    "\n",
    "Since you have used the TF-IDF vectorizer, calculating the dot product will directly give you the cosine similarity score. Therefore, you will use sklearn's `linear_kernel()` instead of `cosine_similarities()` since it is faster.\n",
    "\n",
    "#### DO IT YOURSELF\n",
    "You can try to implement it using the `cosine_similarities()` function yourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're going to define a function that takes in a movie title as an input and outputs a list of the 10 most similar movies. Firstly, for this, you need a reverse mapping of movie titles and DataFrame indices. In other words, you need a mechanism to identify the index of a movie in your `metadata` DataFrame, given its title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Construct a reverse map of indices and movie titles\n",
    "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
    "indices[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now in a good position to define your recommendation function. These are the following steps you'll follow:\n",
    "\n",
    "- Get the index of the movie given its title.\n",
    "- Get the list of cosine similarity scores for that particular movie with all movies. Convert it into a list of tuples where the first element is its position and the second is the similarity score.\n",
    "- Sort the aforementioned list of tuples based on the similarity scores; that is, the second element.\n",
    "- Get the top 10 elements of this list. Ignore the first element as it refers to self (the movie most similar to a particular movie is the movie itself).\n",
    "- Return the titles corresponding to the indices of the top elements.\n",
    "\n",
    "**NOTE**: Above are the programming logic for a function - when you are programming, the hardest part is to come up with the programming logic - I would suggest you study the programming logic above, and cross-compare with the function below to get an idea. Give it some practice so that you can write your own function easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that takes in movie title as input and outputs most similar movies\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Get the index of the movie that matches the title\n",
    "    idx = indices[title]\n",
    "\n",
    "    # Get the pairwsie similarity scores of all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar movies\n",
    "    return metadata['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test our recommender system using a few movies.\n",
    "\n",
    "Let's first test it with `The Dark Knight Rises`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_recommendations('Jumanji')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyone into classics? Let's test it with `The Godfather`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_recommendations('The Godfather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that, while your system has done a decent job of finding movies with similar plot descriptions, the quality of recommendations is not that great. \"The Dark Knight Rises\" returns all Batman movies while it more likely that the people who liked that movie are more inclined to enjoy other Christopher Nolan movies. This is something that cannot be captured by your present system.\n",
    "\n",
    "### Credits, Genres and Keywords Based Recommender\n",
    "It goes without saying that the quality of your recommender would be increased with the usage of better metadata. That is exactly what you are going to do in this section. You are going to build a recommender based on the following metadata: the 3 top actors, the director, related genres and the movie plot keywords.\n",
    "\n",
    "The keywords, cast and crew data is not available in your current dataset so the first step would be to load and merge them into your main DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load keywords and credits\n",
    "credits = pd.read_csv('./data/credits.csv')\n",
    "keywords = pd.read_csv('./data/keywords.csv')\n",
    "\n",
    "# Remove rows with bad IDs.\n",
    "metadata = metadata.drop([19730, 29503, 35587])\n",
    "\n",
    "# Convert IDs to int. Required for merging\n",
    "keywords['id'] = keywords['id'].astype('int')\n",
    "credits['id'] = credits['id'].astype('int')\n",
    "metadata['id'] = metadata['id'].astype('int')\n",
    "\n",
    "# Merge keywords and credits into your main metadata dataframe\n",
    "metadata = metadata.merge(credits, on='id')\n",
    "metadata = metadata.merge(keywords, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the first two movies of your newly merged metadata\n",
    "metadata.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From your new features, cast, crew and keywords, you need to extract the three most important actors, the director and the keywords associated with that movie. Right now, your data is present in the form of \"stringified\" lists. You need to convert them into a form that is usable for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parse the stringified features into their corresponding python objects\n",
    "from ast import literal_eval\n",
    "\n",
    "features = ['cast', 'crew', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(literal_eval)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you write functions that will help you to extract the required information from each feature. First, you'll import the NumPy package to get access to its `NaN` constant. Next, you can use it to write the `get_director()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Numpy \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the director's name from the crew feature. If director is not listed, return NaN\n",
    "def get_director(x):\n",
    "    for i in x:\n",
    "        if i['job'] == 'Director':\n",
    "            return i['name']\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the list top 3 elements or entire list; whichever is more.\n",
    "def get_list(x):\n",
    "    if isinstance(x, list):\n",
    "        names = [i['name'] for i in x]\n",
    "        #Check if more than 3 elements exist. If yes, return only first three. If no, return entire list.\n",
    "        if len(names) > 3:\n",
    "            names = names[:3]\n",
    "        return names\n",
    "\n",
    "    #Return empty list in case of missing/malformed data\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note how if-statement is written in functions - this is the most pythonic way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define new director, cast, genres and keywords features that are in a suitable form.\n",
    "metadata['director'] = metadata['crew'].apply(get_director)\n",
    "\n",
    "features = ['cast', 'keywords', 'genres']\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(get_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print the new features of the first 3 films\n",
    "metadata[['title', 'cast', 'director', 'keywords', 'genres']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step would be to convert the names and keyword instances into lowercase and strip all the spaces between them. This is done so that your vectorizer doesn't count the Johnny of \"Johnny Depp\" and \"Johnny Galecki\" as the same. After this processing step, the aforementioned actors will be represented as \"johnnydepp\" and \"johnnygalecki\" and will be distinct to your vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convert all strings to lower case and strip names of spaces\n",
    "def clean_data(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str.lower(i.replace(\" \", \"\")) for i in x]\n",
    "    else:\n",
    "        #Check if director exists. If not, return empty string\n",
    "        if isinstance(x, str):\n",
    "            return str.lower(x.replace(\" \", \"\"))\n",
    "        else:\n",
    "            return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply clean_data function to your features.\n",
    "features = ['cast', 'keywords', 'director', 'genres']\n",
    "\n",
    "for feature in features:\n",
    "    metadata[feature] = metadata[feature].apply(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now in a position to create your \"metadata soup\", which is a string that contains all the metadata that you want to feed to your vectorizer (namely actors, director and keywords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_soup(x):\n",
    "    return ' '.join(x['keywords']) + ' ' + ' '.join(x['cast']) + ' ' + x['director'] + ' ' + ' '.join(x['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new soup feature\n",
    "metadata['soup'] = metadata.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps are the same as what you did with your plot description based recommender. One important difference is that you use the `CountVectorizer()` instead of TF-IDF. This is because you do not want to down-weight the presence of an actor/director if he or she has acted or directed in relatively more movies. It doesn't make much intuitive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import CountVectorizer and create the count matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(stop_words='english')\n",
    "count_matrix = count.fit_transform(metadata['soup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the Cosine Similarity matrix based on the count_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim2 = cosine_similarity(count_matrix, count_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reset index of your main DataFrame and construct reverse mapping as before\n",
    "metadata = metadata.reset_index()\n",
    "indices = pd.Series(metadata.index, index=metadata['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now reuse your `get_recommendations()` function by passing in the new cosine_sim2 matrix as your second argument.\n",
    "\n",
    "**NOTE**: in the `get_recommendations()` function we defined a default value for the `cosine_sim` parameter; but you can always update it by passing a new value to it. This is common practice in the field.\n",
    "\n",
    "Let's try the same two examples we used above, and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_recommendations('The Dark Knight Rises', cosine_sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_recommendations('The Godfather', cosine_sim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that your recommender has been successful in capturing more information due to more metadata and has given you (arguably) better recommendations. There are, of course, numerous ways of playing with this system in order to improve recommendations.\n",
    "\n",
    "#### Some suggestions:\n",
    "\n",
    "- Introduce a popularity filter: this recommender would take the list of the 30 most similar movies, calculate the weighted ratings (using the IMDB formula from above), sort movies based on this rating and return the top 10 movies.\n",
    "- Other crew members: other crew member names, such as screenwriters and producers, could also be included.\n",
    "- Increasing weight of the director: to give more weight to the director, he or she could be mentioned multiple times in the soup to increase the similarity scores of movies with the same director.\n",
    "\n",
    "You can find these ideas implemented in [this notebook](https://github.com/rounakbanik/movies/blob/master/movies_recommender.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering with Python\n",
    "In this tutorial, you have learnt how to build your very own **Simple** and **Content Based** Movie Recommender Systems. There is also another extremely popular type of recommender known as **collaborative filters**.\n",
    "\n",
    "Collaborative filters can further be classified into two types:\n",
    "\n",
    "- User-based Filtering: these systems recommend products to a user that similar users have liked. For example, let's say Alice and Bob have a similar interest in books (that is, they largely like and dislike the same books). Now, let's say a new book has been launched into the market and Alice has read and loved it. It is therefore, highly likely that Bob will like it too and therefore, the system recommends this book to Bob.\n",
    "\n",
    "- Item-based Filtering: these systems are extremely similar to the content recommendation engine that you built. These systems identify similar items based on how people have rated it in the past. For example, if Alice, Bob and Eve have given 5 stars to The Lord of the Rings and The Hobbit, the system identifies the items as similar. Therefore, if someone buys The Lord of the Rings, the system also recommends The Hobbit to him or her.\n",
    "\n",
    "You will not be building these systems in this tutorial but you are already familiar with most of the ideas required to do so. A good place to start with collaborative filters is by examining the MovieLens dataset, which can be found [here](https://grouplens.org/datasets/movielens/).\n",
    "\n",
    "## Conclusion\n",
    "In this tutorial, you have covered how to build simple as well as content-based recommenders. Hopefully, you are now in a good position to make improvements on the basic systems you built and experiment with other kinds of recommenders (such as collaborative filters). I hope you had as much fun reading this as I had writing. Happy recommending!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
